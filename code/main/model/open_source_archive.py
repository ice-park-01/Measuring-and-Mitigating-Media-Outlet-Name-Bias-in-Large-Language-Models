import os
import sys
import json
# í˜„ì¬ íŒŒì¼ì˜ ë””ë ‰í† ë¦¬ ê²½ë¡œë¥¼ ê¸°ì¤€ìœ¼ë¡œ custom_dataset í´ë”ì˜ ê²½ë¡œë¥¼ ì¶”ê°€
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import torch
import numpy as np

# ì‹œê°í™”ë¥¼ ìœ„í•œ matplotlib ì‚¬ìš©
import matplotlib.pyplot as plt
import seaborn as sns

# XAIë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
import shap
from captum.attr import Saliency, IntegratedGradients, LayerIntegratedGradients
from captum.attr import visualization as viz
        
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GenerationConfig
from custom_dataset.dataset import CustomDatasetAllsides

import numpy as np
import matplotlib.pyplot as plt

  
class OpenSourceModel:
    def __init__(self, model_name: str, cache_dir: str, quantization: bool = True):
        self.model_name = model_name.lower()
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, 
                                                       cache_dir=cache_dir,
                                                       device_map="auto",
                                                       trust_remote_code=True
                                                       )
        
        # pad_token_id ì„¤ì •
        if self.tokenizer.pad_token_id is None:
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
            
        # ê³µí†µ ëª¨ë¸ ë§¤ê°œë³€ìˆ˜
        model_params = {
            "cache_dir": cache_dir,
            "device_map": "auto",
            "trust_remote_code": True,
            "pad_token_id": self.tokenizer.pad_token_id,
            "attn_implementation": "eager"
        }
        
        if quantization:
            if "gemma" in model_name:
                quant_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16) # , 
                model_params.update({
                    "quantization_config": quant_config,
                    "torch_dtype": torch.bfloat16
                })
                self.model = AutoModelForCausalLM.from_pretrained(model_name, **model_params)
            elif "llama" in model_name:
                quant_config = BitsAndBytesConfig(load_in_4bit=True)
                model_params.update({
                    "quantization_config": quant_config,
                    "torch_dtype": torch.bfloat16
                })
                self.model = AutoModelForCausalLM.from_pretrained(model_name, **model_params)
            else:
                quant_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)
                model_params.update({
                    "quantization_config": quant_config
                })
                self.model = AutoModelForCausalLM.from_pretrained(model_name, **model_params)
        else:
            self.model = AutoModelForCausalLM.from_pretrained(model_name, **model_params)
            
        self.lbls_map = {v: k for k, v in self.tokenizer.get_vocab().items()}
        

    def process_question_natural(self, data, news_name: str):
        prompt_text = data.get_natural_prompt(news_name)
        
        inputs = self.tokenizer.encode_plus(prompt_text, return_tensors="pt", return_token_type_ids=False)
        for k, v in inputs.items():
            inputs[k] = v.to(self.model.device)
            
        outputs = self.model(**inputs)
        # print(f"outputs.logits: {outputs.logits}")
        outputs.logits = outputs.logits.to('cpu')
        logits = outputs.logits[0, -1]
        
        probs = logits.float().softmax(dim=-1)

        logprobs_dict = {
            self.lbls_map[i]:
            np.log(probs[i].item()) for i in range(len(self.lbls_map))
        }
                
        # for i, (k, v) in enumerate(logprobs_dict.items()):
        #     if i >= 5:
        #         break
        #     print(k, v)
        
        # Reduce logprobs_dict to only keys with top 50 largest values
        logprobs_dict = {
            k: v for k, v in sorted(
                logprobs_dict.items(),
                key=lambda item: item[1],
                reverse=True
            )[:150]
        }
        
        # GPU ë©”ëª¨ë¦¬ í•´ì œ
        # del inputs, outputs, logits, probs
        # torch.cuda.empty_cache()
        
        return logprobs_dict, data
    
    def process_question_natural_for_order(self, data, news_name: str):
        prompt_text, order = data.get_natural_prompt_for_order(news_name)
        
        # print(f"prompt_text: \n\n{prompt_text[:500]}")
        
        inputs = self.tokenizer.encode_plus(prompt_text, return_tensors="pt", return_token_type_ids=False)
        for k, v in inputs.items():
            inputs[k] = v.to(self.model.device)
            
        outputs = self.model(**inputs)
        # print(f"outputs.logits: {outputs.logits}")
        outputs.logits = outputs.logits.to('cpu')
        logits = outputs.logits[0, -1]
        
        probs = logits.float().softmax(dim=-1)

        logprobs_dict = {
            self.lbls_map[i]:
            np.log(probs[i].item()) for i in range(len(self.lbls_map))
        }
                
        # for i, (k, v) in enumerate(logprobs_dict.items()):
        #     if i >= 5:
        #         break
        #     print(k, v)
        
        # Reduce logprobs_dict to only keys with top 50 largest values
        logprobs_dict = {
            k: v for k, v in sorted(
                logprobs_dict.items(),
                key=lambda item: item[1],
                reverse=True
            )[:150]
        }
        
        # GPU ë©”ëª¨ë¦¬ í•´ì œ
        # del inputs, outputs, logits, probs
        # torch.cuda.empty_cache()
        
        return logprobs_dict, data, order
    
    def process_question_natural_optimization(self, data, prompt: str, news_name: str):
        prompt_text = data.get_natural_prompt_optimization(prompt, news_name)
        
        inputs = self.tokenizer.encode_plus(prompt_text, return_tensors="pt", return_token_type_ids=False)
        for k, v in inputs.items():
            inputs[k] = v.to(self.model.device)
            
        outputs = self.model(**inputs)
        # print(f"outputs.logits: {outputs.logits}")
        outputs.logits = outputs.logits.to('cpu')
        logits = outputs.logits[0, -1]
        
        probs = logits.float().softmax(dim=-1)

        logprobs_dict = {
            self.lbls_map[i]:
            np.log(probs[i].item()) for i in range(len(self.lbls_map))
        }
                
        # for i, (k, v) in enumerate(logprobs_dict.items()):
        #     if i >= 5:
        #         break
        #     print(k, v)
        
        # Reduce logprobs_dict to only keys with top 50 largest values
        logprobs_dict = {
            k: v for k, v in sorted(
                logprobs_dict.items(),
                key=lambda item: item[1],
                reverse=True
            )[:150]
        }
        
        # GPU ë©”ëª¨ë¦¬ í•´ì œ
        # del inputs, outputs, logits, probs
        # torch.cuda.empty_cache()
        
        return logprobs_dict, data
    
    def process_question_summarization(self, data, news_name: str, summary_length: int):
        summary_length_conversion = {
            3: "three",
            5: "five",
            10: "ten"
        }
        prompt_text = data.get_summarization_prompt(news_name, summary_length_conversion[summary_length])
        
        if "qwen" in self.model_name:
            messages = [
                {"role": "system", "content": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant. Follow the user's instructions carefully. Do not include any other text except the answer."},
                {"role": "user", "content": prompt_text}
            ]
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
        elif "phi" in self.model_name:
            messages = [
                {"role": "system", "content": "You are a helpful assistant. Follow the user's instructions carefully. Do not include any other text except the answer."},
                {"role": "user", "content": prompt_text}
            ]
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
        elif "gemma" in self.model_name:
            messages = [
                {"role": "user", "content": "You are a helpful assistant. Follow the user's instructions carefully. Do not include any other text except the answer.\n\n" + prompt_text}
            ]
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
        elif "llama" in self.model_name:
            messages = [
                {"role": "system", "content": "You are a helpful assistant. Follow the user's instructions carefully. Do not include any other text except the answer."},
                {"role": "user", "content": prompt_text}
            ]
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
        else:
            # text = prompt_text + "Only output the summary.\nSummary:"
            text = prompt_text
            
        # print(f"Model input: \n{text}")
        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
        
        outputs = self.model.generate(
            **model_inputs, 
            max_new_tokens=512, 
            temperature=0.3,
            # repetition_penalty=1.1,
            do_sample=True,
            # pad_token_id=self.tokenizer.pad_token_id
        )
        
        generated_ids = [
            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, outputs)
        ]
        response = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)
        
        print(response)
        print("--------------------------------")
        
        return response, data
    
    
    def process_question_natural_xai(self, data, news_name: str, target_token: str = None):
        """
        Saliencyë¥¼ ì‚¬ìš©í•˜ì—¬ news_nameì´ ëª¨ë¸ ì¶”ë¡ ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì •ëŸ‰í™”
        (ê°„ì†Œí™”ëœ ì•ˆì •ì ì¸ ë²„ì „)
        
        Args:
            data: CustomDatasetAllsides ê°ì²´
            news_name: ë‰´ìŠ¤ ì†ŒìŠ¤ ì´ë¦„
            target_token: ë¶„ì„í•  íƒ€ê²Ÿ í† í° (Noneì´ë©´ ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í† í° ì‚¬ìš©)
        
        Returns:
            dict: XAI ë¶„ì„ ê²°ê³¼
        """
        print(f"ğŸ”¬ XAI ë¶„ì„ ì‹œì‘: {news_name}")
        
        # ì›ë³¸ í”„ë¡¬í”„íŠ¸ì™€ ë² ì´ìŠ¤ë¼ì¸ í”„ë¡¬í”„íŠ¸ ìƒì„±
        original_prompt = data.get_natural_prompt(news_name)
        baseline_prompt = data.get_natural_prompt("none")
        
        # í† í°í™”
        original_inputs = self.tokenizer.encode_plus(original_prompt, return_tensors="pt", return_token_type_ids=False)
        baseline_inputs = self.tokenizer.encode_plus(baseline_prompt, return_tensors="pt", return_token_type_ids=False)
        
        # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        for k, v in original_inputs.items():
            original_inputs[k] = v.to(self.model.device)
        for k, v in baseline_inputs.items():
            baseline_inputs[k] = v.to(self.model.device)
        
        # ì›ë³¸ ëª¨ë¸ ì¶”ë¡ ìœ¼ë¡œ í™•ë¥  ë³€í™” ê³„ì‚°
        with torch.no_grad():
            original_outputs = self.model(**original_inputs)
            original_logits = original_outputs.logits[0, -1].to('cpu')
            original_probs = original_logits.float().softmax(dim=-1)
            
            baseline_outputs = self.model(**baseline_inputs)
            baseline_logits = baseline_outputs.logits[0, -1].to('cpu')
            baseline_probs = baseline_logits.float().softmax(dim=-1)
        
        # íƒ€ê²Ÿ í† í° ê²°ì •
        if target_token is None:
            target_token_id = torch.argmax(original_probs).item()
            target_token = self.lbls_map[target_token_id]
        else:
            target_token_id = self.tokenizer.convert_tokens_to_ids(target_token)
        
        print(f"ğŸ¯ ë¶„ì„ íƒ€ê²Ÿ í† í°: '{target_token}' (ID: {target_token_id})")
        print(f"ğŸ“Š íƒ€ê²Ÿ í† í° í™•ë¥  - ì›ë³¸: {original_probs[target_token_id]:.4f}, ë² ì´ìŠ¤ë¼ì¸: {baseline_probs[target_token_id]:.4f}")
        
        # Saliency ë¶„ì„
        try:
            # ì„ë² ë”© ì¤€ë¹„
            original_embeds = self.model.get_input_embeddings()(original_inputs['input_ids'])
            original_embeds.requires_grad_(True)
            
            # Forward function
            def model_forward_saliency(input_embeds):
                outputs = self.model(inputs_embeds=input_embeds)
                return outputs.logits[:, -1, :]
            
            # Saliency ê³„ì‚°
            saliency = Saliency(model_forward_saliency)
            saliency_attr = saliency.attribute(
                original_embeds,
                target=target_token_id,
                abs=False
            )
            
            # í† í° ë‹¨ìœ„ë¡œ saliency ì ìˆ˜ ìš”ì•½ (L2 norm ì‚¬ìš©)
            saliency_per_token = torch.norm(saliency_attr[0], dim=-1)  # [seq_len]
            print("âœ… Saliency ê³„ì‚° ì™„ë£Œ")
            
        except Exception as e:
            print(f"âš ï¸ Saliency ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
            seq_len = original_inputs['input_ids'].shape[1]
            saliency_per_token = torch.zeros(seq_len)
        
        # í† í° ì •ë³´ ì¤€ë¹„
        prompt_tokens = self.tokenizer.tokenize(original_prompt)
        baseline_tokens = self.tokenizer.tokenize(baseline_prompt)
        
        # news_name ë¶€ë¶„ì˜ í† í° ì¸ë±ìŠ¤ ì°¾ê¸°
        for i, (orig_token, base_token) in enumerate(zip(prompt_tokens, baseline_tokens)):
            if orig_token != base_token:
                if news_name_start is None:
                    news_name_start = i
                for j, token in enumerate(prompt_tokens[i:]):
                    if token == base_token:
                        news_name_end = i + j
                        break
                break
        
        # ë²”ìœ„ ê²€ì¦ ë° ì œí•œ
        if news_name_start is not None and news_name_end is not None:
            # ìµœëŒ€ 8í† í°ìœ¼ë¡œ ì œí•œ
            if news_name_end - news_name_start > 8:
                news_name_end = news_name_start + 8
            
            # saliency_per_token ê¸¸ì´ ë‚´ë¡œ ì œí•œ
            max_idx = len(saliency_per_token) - 1
            news_name_start = min(news_name_start, max_idx)
            news_name_end = min(news_name_end, max_idx)
            
            print(f"ğŸ” News name í† í° ë²”ìœ„: {news_name_start}-{news_name_end}")
            print(f"ğŸ” í† í° ë‚´ìš©: {prompt_tokens[news_name_start:news_name_end+1]}")
        else:
            print("âš ï¸ News name í† í°ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì˜í–¥ë ¥ ê³„ì‚°
        total_saliency = torch.abs(saliency_per_token).sum().item()
        
        if news_name_start is not None and news_name_end is not None and news_name_start <= news_name_end:
            news_saliency = torch.abs(saliency_per_token[news_name_start:news_name_end+1]).sum().item()
            saliency_ratio = (news_saliency / total_saliency * 100) if total_saliency > 0 else 0
            
            print(f"\nğŸ“ˆ News Name ì˜í–¥ë„ ë¶„ì„:")
            print(f"   ğŸ” Saliency: {saliency_ratio:.2f}% ({news_saliency:.4f} / {total_saliency:.4f})")
            print(f"   ğŸ“ í† í° ìœ„ì¹˜: {news_name_start}-{news_name_end}")
        else:
            saliency_ratio = 0
            news_saliency = 0
            print(f"âš ï¸ News name ì˜í–¥ë ¥ì„ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # í™•ë¥  ë³€í™”ëŸ‰
        prob_change = original_probs[target_token_id].item() - baseline_probs[target_token_id].item()
        
        # ê²°ê³¼ ì •ë¦¬
        xai_results = {
            'target_token': target_token,
            'target_token_id': target_token_id,
            'original_probability': original_probs[target_token_id].item(),
            'baseline_probability': baseline_probs[target_token_id].item(),
            'probability_change': prob_change,
            
            # Saliency scores
            'saliency_scores': saliency_per_token.cpu().numpy(),
            'saliency_abs_scores': torch.abs(saliency_per_token).cpu().numpy(),
            
            # news_name í† í° ìœ„ì¹˜
            'news_name_token_range': (news_name_start, news_name_end),
            
            # í† í° ì •ë³´
            'tokens': prompt_tokens,
            'token_ids': original_inputs['input_ids'][0].cpu().numpy(),
            
            # news_name ì˜í–¥ë ¥ ìš”ì•½
            'news_name_impact': {
                'saliency_sum': float(news_saliency),
                'saliency_mean': float(news_saliency / (news_name_end - news_name_start + 1)) if news_name_start is not None and news_name_end is not None and news_name_start <= news_name_end else 0,
            },
            
            # ì˜í–¥ ë¹„ìœ¨
            'news_name_influence_ratio': {
                'saliency_ratio': saliency_ratio,
                'probability_change_ratio': abs(prob_change) * 100,  # í™•ë¥  ë³€í™”ë¥¼ ë°±ë¶„ìœ¨ë¡œ
                'average_ratio': (saliency_ratio + abs(prob_change) * 100) / 2,
            },
            
            # ì „ì²´ ì¤‘ìš”ë„ ì ìˆ˜
            'total_importance': {
                'total_saliency': total_saliency,
            }
        }
        
        return xai_results, data
    
    def analyze_news_name_impact(self, xai_results, save_path: str = None):
        """
        XAI ë¶„ì„ ê²°ê³¼ì—ì„œ news_nameì˜ ì˜í–¥ì„ ë¶„ì„í•˜ê³  ì‹œê°í™”
        
        Args:
            xai_results: process_question_natural_xaiì˜ ê²°ê³¼
            save_path: ê²°ê³¼ë¥¼ ì €ì¥í•  ê²½ë¡œ (Noneì´ë©´ ì €ì¥í•˜ì§€ ì•ŠìŒ)
        """
        # 1. ê¸°ë³¸ ì •ë³´ ì¶œë ¥
        print("\n" + "="*50)
        print("ğŸ”¬ XAI ë¶„ì„ ê²°ê³¼ ìš”ì•½")
        print("="*50)
        print(f"ğŸ¯ íƒ€ê²Ÿ í† í°: {xai_results['target_token']}")
        print(f"ğŸ“Š ì›ë³¸ í™•ë¥ : {xai_results['original_probability']:.4f}")
        print(f"ğŸ“Š ë² ì´ìŠ¤ë¼ì¸ í™•ë¥ : {xai_results['baseline_probability']:.4f}")
        print(f"ğŸ“ˆ í™•ë¥  ë³€í™”: {xai_results['probability_change']:.4f}")
        
        # 2. news_name ì˜í–¥ ë¹„ìœ¨ ì¶œë ¥ (í•µì‹¬ ì •ë³´)
        ratio = xai_results['news_name_influence_ratio']
        print("\n" + "="*50)
        print("ğŸ“Š News Nameì´ ì „ì²´ ì…ë ¥ì—ì„œ ì°¨ì§€í•˜ëŠ” ì˜í–¥ ë¹„ìœ¨")
        print("="*50)
        print(f"ğŸ” Saliency ë°©ë²•: {ratio['saliency_ratio']:.2f}%")
        print(f"ğŸ” í™•ë¥  ë³€í™”: {ratio['probability_change_ratio']:.2f}%")
        print(f"ğŸ“Š í‰ê·  ì˜í–¥ ë¹„ìœ¨: {ratio['average_ratio']:.2f}%")
        
        # 3. news_name ì ˆëŒ€ ì˜í–¥ë ¥ ì¶œë ¥
        impact = xai_results['news_name_impact']
        print("\n" + "="*30)
        print("ğŸ“ˆ News Name ì ˆëŒ€ ì˜í–¥ë ¥")
        print("="*30)
        print(f"Saliency Sum: {impact['saliency_sum']:.4f}")
        print(f"Saliency Mean: {impact['saliency_mean']:.4f}")
        print(f"í™•ë¥  ë³€í™”: {xai_results['probability_change']:.4f}")
        
        # 4. í† í°ë³„ ì¤‘ìš”ë„ ì‹œê°í™”
        tokens = xai_results['tokens']
        news_start, news_end = xai_results['news_name_token_range']
        
        if news_start is not None and news_end is not None:
            # í† í°ê³¼ ì ìˆ˜ ë°°ì—´ì˜ ê¸¸ì´ ë§ì¶¤
            saliency_scores = xai_results['saliency_abs_scores']
            
            # ì°¨ì› í™•ì¸ ë° ì¡°ì •
            print(f"ğŸ”§ ì‹œê°í™” ì°¨ì› ì •ë³´: tokens={len(tokens)}, saliency_scores={saliency_scores.shape}")
            
            # ìµœì†Œ ê¸¸ì´ë¡œ ë§ì¶¤
            min_len = min(len(tokens), len(saliency_scores))
            tokens_viz = tokens[:min_len]
            saliency_scores_viz = saliency_scores[:min_len]
            
            # news_name ë²”ìœ„ë„ ì¡°ì •
            news_start_viz = min(news_start, min_len - 1)
            news_end_viz = min(news_end, min_len - 1)
            
            print(f"ğŸ”§ ì¡°ì •ëœ ê¸¸ì´: {min_len}, news_range: {news_start_viz}-{news_end_viz}")
            
            # ì „ì²´ í† í°ì— ëŒ€í•œ ì¤‘ìš”ë„ í”Œë¡¯
            fig, axes = plt.subplots(2, 1, figsize=(18, 10))
            
            # Saliency scores
            axes[0].bar(range(len(tokens_viz)), saliency_scores_viz, alpha=0.7, color='blue')
            if news_start_viz <= news_end_viz and news_end_viz < len(saliency_scores_viz):
                axes[0].bar(range(news_start_viz, news_end_viz+1), saliency_scores_viz[news_start_viz:news_end_viz+1], 
                           alpha=0.9, color='red', label=f'News Name Tokens ({ratio["saliency_ratio"]:.1f}%)')
            axes[0].set_title('Saliency Scores by Token')
            axes[0].set_ylabel('Absolute Saliency Score')
            axes[0].legend()
            
            # ì˜í–¥ ë¹„ìœ¨ ë¹„êµ ì°¨íŠ¸
            methods = ['Saliency', 'Probability Change']
            ratios_vals = [ratio['saliency_ratio'], ratio['probability_change_ratio']]
            colors_ratio = ['blue', 'green']
            
            bars = axes[1].bar(methods, ratios_vals, color=colors_ratio, alpha=0.7)
            axes[1].set_title(f'News Name Influence Analysis (Average: {ratio["average_ratio"]:.1f}%)')
            axes[1].set_ylabel('Influence (%)')
            axes[1].set_ylim(0, max(max(ratios_vals), 1.0) * 1.2)
            
            # ë§‰ëŒ€ ìœ„ì— ìˆ˜ì¹˜ í‘œì‹œ
            for bar, ratio_val in zip(bars, ratios_vals):
                height = bar.get_height()
                axes[1].text(bar.get_x() + bar.get_width()/2., height + 0.1,
                           f'{ratio_val:.1f}%', ha='center', va='bottom', fontweight='bold')
            
            # xì¶• ë ˆì´ë¸” ì„¤ì • (ì²« ë²ˆì§¸ í”Œë¡¯ë§Œ)
            axes[0].set_xticks(range(len(tokens_viz)))
            axes[0].set_xticklabels(tokens_viz, rotation=45, ha='right', fontsize=8)
            axes[0].set_xlabel('Token Index')
            
            plt.tight_layout()
            
            if save_path:
                plt.savefig(f"{save_path}_token_importance_with_ratio.png", dpi=300, bbox_inches='tight')
            
            plt.show()
        
        # 5. ì¢…í•© ì˜í–¥ë ¥ ë¶„ì„
        influence_score = self._calculate_simple_influence_score(xai_results)
        
        print("\n" + "="*40)
        print("ğŸ† ì¢…í•© ì˜í–¥ë ¥ ë¶„ì„")
        print("="*40)
        print(f"ğŸ“Š ì¢…í•© ì˜í–¥ë ¥ ì ìˆ˜: {influence_score:.4f}")
        print(f"ğŸ“Š í‰ê·  ì˜í–¥ ë¹„ìœ¨: {ratio['average_ratio']:.2f}%")
        
        # ì˜í–¥ë„ í•´ì„
        if ratio['average_ratio'] > 15:
            interpretation = "ğŸ”´ ë§¤ìš° ë†’ì€ ì˜í–¥"
        elif ratio['average_ratio'] > 8:
            interpretation = "ğŸŸ¡ ë†’ì€ ì˜í–¥"
        elif ratio['average_ratio'] > 3:
            interpretation = "ğŸŸ¢ ì¤‘ê°„ ì˜í–¥"
        else:
            interpretation = "âšª ë‚®ì€ ì˜í–¥"
        
        print(f"ğŸ“ ì˜í–¥ë„ í•´ì„: {interpretation}")
        print("="*40)
        
        return influence_score, ratio['average_ratio']
    
    def _calculate_simple_influence_score(self, xai_results):
        """
        ê°„ì†Œí™”ëœ ì˜í–¥ë ¥ ì ìˆ˜ ê³„ì‚° (Saliency ê¸°ë°˜)
        """
        impact = xai_results['news_name_impact']
        prob_change = abs(xai_results['probability_change'])
        
        # Saliency ê¸°ë°˜ ì ìˆ˜ (ì •ê·œí™”)
        saliency_score = impact['saliency_mean']
        max_saliency = max(xai_results['saliency_abs_scores']) if len(xai_results['saliency_abs_scores']) > 0 else 1e-8
        normalized_saliency = saliency_score / max_saliency if max_saliency > 0 else 0
        
        # í™•ë¥  ë³€í™” ì ìˆ˜ (ì •ê·œí™”)
        normalized_prob = min(prob_change * 10, 1.0)  # í™•ë¥  ë³€í™”ë¥¼ 0-1ë¡œ ì •ê·œí™”
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì¢…í•© ì ìˆ˜ ê³„ì‚°
        weights = {'saliency': 0.7, 'prob_change': 0.3}
        total_score = (
            weights['saliency'] * normalized_saliency +
            weights['prob_change'] * normalized_prob
        )
        
        return total_score
    
    def _calculate_influence_score(self, xai_results):
        """
        ì—¬ëŸ¬ XAI ë°©ë²•ì˜ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ news_nameì˜ ì˜í–¥ë ¥ ì ìˆ˜ ê³„ì‚°
        """
        impact = xai_results['news_name_impact']
        
        # ê° ë°©ë²•ë³„ ì •ê·œí™”ëœ ì ìˆ˜ ê³„ì‚°
        saliency_score = impact['saliency_mean']
        shap_score = impact['shap_mean']

        # í™•ë¥  ë³€í™”ë„ ê³ ë ¤
        prob_change = abs(xai_results['probability_change'])
        
        original_scores = {
            'saliency': saliency_score,
            'shap': shap_score,
            'prob_change': prob_change
        }
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì¢…í•© ì ìˆ˜ ê³„ì‚°
        # ê° ë°©ë²•ì˜ ì¤‘ìš”ë„ë¥¼ ê³ ë ¤í•œ ê°€ì¤‘ì¹˜ ì„¤ì •
        weights = {
            'saliency': 0.3,
            'shap': 0.3,
            'prob_change': 0.1
        }
        
        # ì ìˆ˜ ì •ê·œí™” (0-1 ë²”ìœ„ë¡œ)
        max_saliency = max(xai_results['saliency_abs_scores']) if len(xai_results['saliency_abs_scores']) > 0 else 1e-8
        max_shap = max(np.abs(xai_results['shap_values'])) if len(xai_results['shap_values']) > 0 else 1e-8
        
        normalized_saliency = saliency_score / max_saliency if max_saliency > 0 else 0
        normalized_shap = shap_score / max_shap if max_shap > 0 else 0
        normalized_prob = min(prob_change * 10, 1.0)  # í™•ë¥  ë³€í™”ë¥¼ 0-1ë¡œ ì •ê·œí™”
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        total_score = (
            weights['saliency'] * normalized_saliency +
            weights['shap'] * normalized_shap +
            weights['prob_change'] * normalized_prob
        )
        
        return total_score, original_scores


class OpenSourceReasoningModel:
    def __init__(self, model_name: str, cache_dir: str, quantization: bool = True):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, 
                                                       cache_dir=cache_dir,
                                                       device_map="auto",
                                                       trust_remote_code=True
                                                       )
        
        # pad_token_id ì„¤ì •
        if self.tokenizer.pad_token_id is None:
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
            
        # ê³µí†µ ëª¨ë¸ ë§¤ê°œë³€ìˆ˜
        model_params = {
            "cache_dir": cache_dir,
            "device_map": "auto",
            "trust_remote_code": True,
            "pad_token_id": self.tokenizer.pad_token_id  # ì—¬ê¸°ì— pad_token_id ì¶”ê°€
        }
        
        if quantization:
            if "gemma" in model_name:
                quant_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)
                model_params.update({
                    "quantization_config": quant_config,
                    "torch_dtype": torch.bfloat16
                })
                self.model = AutoModelForCausalLM.from_pretrained(model_name, **model_params)
            else:
                quant_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)
                model_params.update({
                    "quantization_config": quant_config
                })
                self.model = AutoModelForCausalLM.from_pretrained(model_name, **model_params)
        else:
            self.model = AutoModelForCausalLM.from_pretrained(model_name, **model_params)
            
        self.lbls_map = {v: k for k, v in self.tokenizer.get_vocab().items()}
        self.generation_config = GenerationConfig(
            max_new_tokens=32768,
            temperature=0.6,
            top_p=0.95,
            min_p=0.0,
            top_k=30,
            do_sample=True,
            pad_token_id=self.tokenizer.pad_token_id  # generation_configì—ë„ pad_token_id ì¶”ê°€
        )
        

    def process_question_natural(self, data, news_name: str):
        prompt_text = data.get_reasoning_prompt(news_name)
        
        messages = [
            {"role": "user", "content": prompt_text}
        ]
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
        generated_ids = self.model.generate(**model_inputs, generation_config=self.generation_config)

        generated_ids = [
            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]
        
        response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        extracted_answer = response.split("</think>")[1].strip()
        extracted_answer = json.loads(extracted_answer)["answer"]
        
        return extracted_answer, data


    
if __name__ == "__main__":
    hf_token = "Your HF Token"
    os.environ["HF_TOKEN"] = hf_token
    os.environ["CUDA_VISIBLE_DEVICES"] = "0"
    model = OpenSourceModel("meta-llama/Llama-3.1-8B-Instruct", "/nas/.cache/huggingface/")
    dataset = CustomDatasetAllsides("../data/allsides/Article-Bias-Prediction/data/jsons", "meta-llama/Llama-3.1-8B-Instruct", "itself")
    print(model.predict(dataset[0]))