# -*- coding: utf-8 -*-
import os, json, datetime, itertools
from collections import defaultdict, Counter
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# -------------------------------------------------
# 0. ë°ì´í„° ë¡œë“œ ë° ì´ˆê¸°í™”
# -------------------------------------------------
# ëª¨ë¸ ë° ì•„í‹°í´ ë©”íƒ€ë°ì´í„° ì´ˆê¸°í™”
all_models = {}
article_meta = {}

# ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
def load_model_data(model_path, model_name):
    """ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ íŒŒì¼ ë¡œë“œ"""
    with open(model_path, 'r') as f:
        model_data = json.load(f)
    all_models[model_name] = model_data
    
def load_article_metadata(metadata_path):
    """ì•„í‹°í´ ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
    with open(metadata_path, 'r') as f:
        metadata = json.load(f)
    global article_meta
    article_meta = metadata

# ìƒ˜í”Œ ë°ì´í„° ìƒì„± (ì‹¤ì œ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° í…ŒìŠ¤íŠ¸ìš©)
def create_sample_data():
    # ìƒ˜í”Œ ì•„í‹°í´ ID ëª©ë¡
    article_ids = [f"art_{i}" for i in range(10)]
    
    # ì•„í‹°í´ ë©”íƒ€ë°ì´í„° ìƒ˜í”Œ ìƒì„±
    for art_id in article_ids:
        # ë¬´ìž‘ìœ„ë¡œ ë ˆì´ë¸” í• ë‹¹
        label = np.random.choice(['left', 'lean_left', 'center', 'lean_right', 'right'])
        article_meta[art_id] = {
            'label': label,
            'topic': np.random.choice(['ì •ì¹˜', 'ê²½ì œ', 'ì‚¬íšŒ', 'êµ­ì œ']),
            'source': np.random.choice(['Aì‹ ë¬¸', 'Bë°©ì†¡', 'Cë§¤ì²´', 'Dí¬í„¸']),
            'bias_text': np.random.choice(['hp', 'non-hp'])
        }
    
    # ëª¨ë¸ ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    model_names = ['model_A', 'model_B', 'model_C']
    for model in model_names:
        model_data = {}
        for art_id in article_ids:
            # ê¸°ë³¸ ì˜ˆì¸¡ê³¼ ë¯¸ë””ì–´ ë°”ì´ì–´ìŠ¤ì— ë”°ë¥¸ ì˜ˆì¸¡ ìƒì„±
            base_pred = np.random.choice(['left', 'lean_left', 'center', 'lean_right', 'right'])
            model_data[art_id] = {
                'baseline_pred': base_pred
            }
            
            for bias in ['left', 'lean_left', 'center', 'lean_right', 'right']:
                # ê° ë¯¸ë””ì–´ ë°”ì´ì–´ìŠ¤ì— ëŒ€í•œ ì˜ˆì¸¡ ìƒì„±
                model_data[art_id][f'{bias}_pred'] = np.random.choice(['left', 'lean_left', 'center', 'lean_right', 'right'])
        
        all_models[model] = model_data

# ìƒ˜í”Œ ë°ì´í„° ìƒì„± (ì‹¤ì œ ë°ì´í„°ë¡œ ëŒ€ì²´í•˜ì„¸ìš”)
create_sample_data()

# -------------------------------------------------
# 1. ì¤€ë¹„: ìˆœì„œì²™ë„ & í—¬í¼
# -------------------------------------------------
ORDER = {'left':-2, 'lean_left':-1, 'center':0,
         'lean_right':1, 'right':2}

def classify_case(base, new, media):
    """Neutral / Confirmation / Amplify / Attenuate / Reversal"""
    Î” = ORDER[new] - ORDER[base]
    dir_media = np.sign(ORDER[media])

    if Î” == 0:
        return 'confirmation' if ORDER[base]*dir_media > 0 else 'neutral'
    if Î”*dir_media > 0:              # ê°™ì€ ë°©í–¥ ì´ë™
        return 'amplify'   if abs(Î”) >= 1 else 'attenuate'
    return 'reversal'

def correction_or_distortion(base, new, label):
    """ì •ë‹µ(label) ë°©í–¥ìœ¼ë¡œ ê°€ê¹Œì›Œì¡ŒëŠ”ì§€"""
    dist_base = abs(ORDER[label] - ORDER[base])
    dist_new  = abs(ORDER[label] - ORDER[new])
    if dist_new < dist_base:
        return 'correction'
    if dist_new > dist_base:
        return 'distortion'
    return 'same'

# -------------------------------------------------
# 2. ë©”ì¸ ë£¨í”„ â€“ ê¸°ì‚¬ Ã— ëª¨ë¸ Ã— media-bias
# -------------------------------------------------
def analyze_bias_cases():
    bias_list   = ['left','lean_left','center','lean_right','right']
    records     = []             # ìµœì¢… í–‰ë³„ ë ˆì½”ë“œ

    for model, article_dict in all_models.items():
        for art_id, res in article_dict.items():
            base_pred = res['baseline_pred']
            meta       = article_meta[art_id]
            art_label  = meta['label']
            topic      = meta.get('topic','N/A')
            source     = meta.get('source','N/A')
            hyperflag  = meta.get('bias_text','non-hp')

            for media in bias_list:
                new_pred = res[f'{media}_pred']
                case     = classify_case(base_pred, new_pred, media)
                impact   = correction_or_distortion(base_pred, new_pred, art_label)
                records.append({
                    'Model': model,
                    'ArticleID': art_id,
                    'MediaBias': media,
                    'ArticleLabel': art_label,
                    'BaselinePred': base_pred,
                    'NewPred': new_pred,
                    'Case': case,
                    'Impact': impact,
                    'Topic': topic,
                    'Source': source,
                    'HP': hyperflag
                })

    df = pd.DataFrame(records)
    out_dir = '../analyze_result/bias_case_analysis'
    os.makedirs(out_dir, exist_ok=True)
    df.to_csv(f'{out_dir}/case_level_long.csv', index=False)
    print("ðŸ”¹ case_level_long.csv ì €ìž¥ ì™„ë£Œ")
    
    return df

# -------------------------------------------------
# 3. ì§‘ê³„ í…Œì´ë¸” â€“ ëª¨ë¸ Ã— MediaBias Ã— Case
# -------------------------------------------------
def create_pivot_table(df):
    pivot = (df.groupby(['Model','MediaBias','Case'])
            .size()
            .reset_index(name='Count'))
    out_dir = '../analyze_result/bias_case_analysis'
    pivot.to_csv(f'{out_dir}/case_counts.csv', index=False)
    return pivot

# -------------------------------------------------
# 4. ë¹„ìœ¨ ížˆíŠ¸ë§µ (Amplify + Reversal) / Total
# -------------------------------------------------
def create_heatmap(pivot):
    effect_cols = ['amplify','reversal','attenuate','confirmation','neutral']
    
    ratio_df = (pivot.pivot_table(index=['Model','MediaBias'],
                                columns='Case',
                                values='Count',
                                fill_value=0)
                    .reset_index())
    ratio_df['Total'] = ratio_df[effect_cols].sum(axis=1)
    ratio_df['Amp+Rev_Ratio'] = (ratio_df['amplify'] + ratio_df['reversal']) / ratio_df['Total']
    heat = ratio_df.pivot(index='MediaBias', columns='Model', values='Amp+Rev_Ratio')

    plt.figure(figsize=(12,4))
    sns.heatmap(heat, annot=True, fmt='.2f', cmap='Reds', cbar_kws={'label':'Amp+Rev ratio'})
    plt.title('Bias-Sensitive Ratio (Amplify+Reversal) by Model / MediaBias')
    out_dir = '../analyze_result/bias_case_analysis'
    plt.savefig(f'{out_dir}/heatmap_amp_rev_ratio.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    return ratio_df

# -------------------------------------------------
# 5. ì£¼ì œÂ·ì¶œì²˜ Over-representation (Amplify vs Neutral)
# -------------------------------------------------
def top_overrep(df, field, top_n=5):
    bias_list = ['left','lean_left','center','lean_right','right']
    out_rows = []
    for model, media in itertools.product(df['Model'].unique(), bias_list):
        sub = df[(df.Model==model)&(df.MediaBias==media)]
        if sub.empty: continue
        # ë¶„í• 
        aff  = sub[sub.Case=='amplify'][field].tolist()
        neu  = sub[sub.Case=='neutral'][field].tolist()
        if not aff or not neu: continue
        cnt_aff, cnt_neu = Counter(aff), Counter(neu)
        tot_aff, tot_neu = len(aff), len(neu)
        over_scores = {
            k: (cnt_aff.get(k,0)/tot_aff) /
               max(cnt_neu.get(k,1)/tot_neu, 1e-3)
            for k in set(cnt_aff)|set(cnt_neu)
        }
        for k, sc in sorted(over_scores.items(), key=lambda x: x[1], reverse=True)[:top_n]:
            out_rows.append({
                'Model': model, 'MediaBias': media,
                'Field': field, 'Value': k, 'Overrep': sc
            })
    return pd.DataFrame(out_rows)

def analyze_overrep(df):
    topic_over  = top_overrep(df, 'Topic',  3)
    source_over = top_overrep(df, 'Source', 3)
    out_dir = '../analyze_result/bias_case_analysis'
    topic_over .to_csv(f'{out_dir}/overrep_topic.csv',  index=False)
    source_over.to_csv(f'{out_dir}/overrep_source.csv', index=False)
    print("ðŸ”¹ overrep_topic.csv / overrep_source.csv ì €ìž¥ ì™„ë£Œ")
    return topic_over, source_over

# -------------------------------------------------
# 6. ìƒ˜í”Œ ì¶”ì¶œ: ìµœëŒ€ Amplify & Distortion
# -------------------------------------------------
def extract_samples(df):
    top_amp = (df[df.Case=='amplify']
              .assign(Move=lambda r: abs(ORDER[r.NewPred]-ORDER[r.BaselinePred]))
              .sort_values('Move', ascending=False)
              .head(20))
    out_dir = '../analyze_result/bias_case_analysis'
    top_amp.to_csv(f'{out_dir}/top20_amplify_cases.csv', index=False)

    worst_dist = df[df.Impact=='distortion'].head(20)
    worst_dist.to_csv(f'{out_dir}/worst20_distortion.csv', index=False)
    return top_amp, worst_dist

# -------------------------------------------------
# ë©”ì¸ ì‹¤í–‰
# -------------------------------------------------
def main():
    # 1. ë°ì´í„° ë¡œë“œ
    # ì‹¤ì œ ë°ì´í„° ë¡œë“œ ì½”ë“œ (ê°€ëŠ¥í•˜ë‹¤ë©´ ì‚¬ìš©)
    # load_model_data("ê²½ë¡œ", "ëª¨ë¸ëª…")
    # load_article_metadata("ê²½ë¡œ")
    
    # 2. ë¶„ì„ ì‹¤í–‰
    df = analyze_bias_cases()
    pivot = create_pivot_table(df)
    ratio_df = create_heatmap(pivot)
    topic_over, source_over = analyze_overrep(df)
    top_amp, worst_dist = extract_samples(df)
    
    print("âœ… ë¶„ì„ Â· ìš”ì•½ Â· ì‹œê°í™” ì™„ë£Œ -> ../analyze_result/bias_case_analysis")

if __name__ == "__main__":
    main() 